{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDW1lGz0JLPx"
   },
   "source": [
    "#FEDERATED LEARNING FINAL PROJECT\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfsSUzsHHhcw"
   },
   "source": [
    "**Author** \n",
    ": Ateniola Oluwatobi Victor\n",
    "\n",
    "**Objective** : My implementation of the Final project in the federated Learning section of the Secure and Private AI Scholarship Challenge Nanodegree Program using the MNIST digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0gXPFf6zVT61",
    "outputId": "9db63e89-d366-4bd7-9429-5ece7fc28516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syft"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [7 lines of output]\n",
      "      C:\\Python\\Python 3.10.6\\lib\\site-packages\\setuptools\\config\\setupcfg.py:463: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n",
      "        warnings.warn(msg, warning_class)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'netifaces' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for netifaces\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for netifaces did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [9 lines of output]\n",
      "      C:\\Python\\Python 3.10.6\\lib\\site-packages\\setuptools\\config\\setupcfg.py:463: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n",
      "        warnings.warn(msg, warning_class)\n",
      "      running install\n",
      "      C:\\Python\\Python 3.10.6\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'netifaces' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> netifaces\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading syft-0.6.0-py2.py3-none-any.whl (606 kB)\n",
      "     ------------------------------------ 607.0/607.0 kB 954.8 kB/s eta 0:00:00\n",
      "Collecting PyNaCl==1.4.0\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-win_amd64.whl (206 kB)\n",
      "     ------------------------------------ 206.2/206.2 kB 694.8 kB/s eta 0:00:00\n",
      "Collecting typing-extensions==4.0.0\n",
      "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting loguru==0.5.3\n",
      "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.2/57.2 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting werkzeug==2.0.2\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "     ------------------------------------ 288.9/288.9 kB 938.3 kB/s eta 0:00:00\n",
      "Collecting gevent==21.8.0\n",
      "  Downloading gevent-21.8.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 375.6 kB/s eta 0:00:00\n",
      "Collecting PyJWT==2.3.0\n",
      "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting pyarrow==6.0.0\n",
      "  Downloading pyarrow-6.0.0-cp310-cp310-win_amd64.whl (15.5 MB)\n",
      "     -------------------------------------- 15.5/15.5 MB 835.9 kB/s eta 0:00:00\n",
      "Collecting ascii-magic==1.6\n",
      "  Downloading ascii_magic-1.6-py3-none-any.whl (7.6 kB)\n",
      "Collecting forbiddenfruit==0.1.4\n",
      "  Downloading forbiddenfruit-0.1.4.tar.gz (43 kB)\n",
      "     ---------------------------------------- 43.8/43.8 kB 2.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pymbolic==2021.1\n",
      "  Downloading pymbolic-2021.1.tar.gz (84 kB)\n",
      "     ---------------------------------------- 84.5/84.5 kB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sympy==1.9\n",
      "  Downloading sympy-1.9-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 737.7 kB/s eta 0:00:00\n",
      "Collecting requests==2.26.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "     -------------------------------------- 62.3/62.3 kB 553.3 kB/s eta 0:00:00\n",
      "Collecting requests-toolbelt==0.9.1\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     -------------------------------------- 54.3/54.3 kB 282.5 kB/s eta 0:00:00\n",
      "Collecting tqdm==4.62.3\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 76.2/76.2 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata==4.8.2\n",
      "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting numpy==1.21.4\n",
      "  Downloading numpy-1.21.4-cp310-cp310-win_amd64.whl (14.0 MB)\n",
      "     ---------------------------------------- 14.0/14.0 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting pydantic[email]==1.8.2\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.0/126.0 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting syft\n",
      "  Downloading syft-0.5.1-py2.py3-none-any.whl (448 kB)\n",
      "     -------------------------------------- 448.6/448.6 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting cryptography>=3.4.7\n",
      "  Downloading cryptography-38.0.3-cp36-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (2.28.1)\n",
      "Collecting requests-toolbelt\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     -------------------------------------- 54.5/54.5 kB 942.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (1.4.3)\n",
      "Collecting loguru\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (5.2.0)\n",
      "Collecting PyNaCl\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-win_amd64.whl (212 kB)\n",
      "     ------------------------------------ 212.1/212.1 kB 807.6 kB/s eta 0:00:00\n",
      "Collecting dpcontracts\n",
      "  Downloading dpcontracts-0.6.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nest-asyncio in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (1.5.5)\n",
      "Collecting flask<2.0.0,>=1.1.2\n",
      "  Downloading Flask-1.1.4-py2.py3-none-any.whl (94 kB)\n",
      "     -------------------------------------- 94.6/94.6 kB 897.4 kB/s eta 0:00:00\n",
      "Collecting PyYAML>=5.4.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "     -------------------------------------- 55.3/55.3 kB 577.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (21.3)\n",
      "Requirement already satisfied: protobuf<=3.20 in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (3.19.4)\n",
      "Collecting sqlitedict\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "     ---------------------------------------- 46.3/46.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wrapt in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (1.14.1)\n",
      "Collecting aiortc\n",
      "  Downloading aiortc-1.3.2-cp310-cp310-win_amd64.whl (992 kB)\n",
      "     -------------------------------------- 992.6/992.6 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (4.3.0)\n",
      "Collecting syft-proto\n",
      "  Downloading syft_proto-0.5.3-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 66.0/66.0 kB 715.2 kB/s eta 0:00:00\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-10.0.0-cp310-cp310-win_amd64.whl (19.9 MB)\n",
      "     -------------------------------------- 19.9/19.9 MB 798.9 kB/s eta 0:00:00\n",
      "Collecting PyJWT==1.7.1\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting names\n",
      "  Downloading names-0.3.0.tar.gz (789 kB)\n",
      "     -------------------------------------- 789.1/789.1 kB 1.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\python 3.10.6\\lib\\site-packages (from syft) (1.1.3)\n",
      "Collecting pyOpenSSL\n",
      "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.0/57.0 kB 599.7 kB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp310-cp310-win_amd64.whl (167.3 MB)\n",
      "     ------------------------------------ 167.3/167.3 MB 829.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python\\python 3.10.6\\lib\\site-packages (from cryptography>=3.4.7->syft) (1.15.1)\n",
      "Collecting Werkzeug<2.0,>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "     ------------------------------------ 298.6/298.6 kB 355.2 kB/s eta 0:00:00\n",
      "Collecting itsdangerous<2.0,>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Jinja2<3.0,>=2.10.1\n",
      "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
      "     -------------------------------------- 125.7/125.7 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting click<8.0,>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.8/82.8 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting av<10.0.0,>=9.0.0\n",
      "  Downloading av-9.2.0-cp310-cp310-win_amd64.whl (24.4 MB)\n",
      "     -------------------------------------- 24.4/24.4 MB 856.5 kB/s eta 0:00:00\n",
      "Collecting pylibsrtp>=0.5.6\n",
      "  Downloading pylibsrtp-0.7.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pyee>=9.0.0\n",
      "  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\n",
      "Collecting aioice<0.8.0,>=0.7.5\n",
      "  Downloading aioice-0.7.6-py3-none-any.whl (23 kB)\n",
      "Collecting google-crc32c>=1.1\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Collecting win32-setctime>=1.0.0\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\python\\python 3.10.6\\lib\\site-packages (from loguru->syft) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python\\python 3.10.6\\lib\\site-packages (from packaging->syft) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python\\python 3.10.6\\lib\\site-packages (from pandas->syft) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python 3.10.6\\lib\\site-packages (from pandas->syft) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python\\python 3.10.6\\lib\\site-packages (from pandas->syft) (1.23.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python 3.10.6\\lib\\site-packages (from requests->syft) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sachin\\appdata\\roaming\\python\\python310\\site-packages (from requests->syft) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python 3.10.6\\lib\\site-packages (from requests->syft) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python 3.10.6\\lib\\site-packages (from requests->syft) (1.26.12)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python\\python 3.10.6\\lib\\site-packages (from scikit-learn->syft) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python 3.10.6\\lib\\site-packages (from scikit-learn->syft) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\python\\python 3.10.6\\lib\\site-packages (from scikit-learn->syft) (1.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python\\python 3.10.6\\lib\\site-packages (from torchvision->syft) (9.2.0)\n",
      "Collecting dnspython>=2.0.0\n",
      "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
      "     -------------------------------------- 269.1/269.1 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting netifaces\n",
      "  Downloading netifaces-0.11.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pycparser in c:\\python\\python 3.10.6\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.7->syft) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\python\\python 3.10.6\\lib\\site-packages (from Jinja2<3.0,>=2.10.1->flask<2.0.0,>=1.1.2->syft) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python 3.10.6\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->syft) (1.16.0)\n",
      "Building wheels for collected packages: forbiddenfruit, dpcontracts, names, sqlitedict, netifaces\n",
      "  Building wheel for forbiddenfruit (setup.py): started\n",
      "  Building wheel for forbiddenfruit (setup.py): finished with status 'done'\n",
      "  Created wheel for forbiddenfruit: filename=forbiddenfruit-0.1.4-py3-none-any.whl size=21792 sha256=601ce500875fbab0e1447a7db2b4c7629f4a38cda24d0e37009db2a0d1caea26\n",
      "  Stored in directory: c:\\users\\sachin\\appdata\\local\\pip\\cache\\wheels\\80\\70\\bf\\aa982a6b9e2c007a1d444743074a9405c144809d1110cd7612\n",
      "  Building wheel for dpcontracts (setup.py): started\n",
      "  Building wheel for dpcontracts (setup.py): finished with status 'done'\n",
      "  Created wheel for dpcontracts: filename=dpcontracts-0.6.0-py3-none-any.whl size=13085 sha256=98b6514dcd40ad34fc0a9642673d17748fb028921bb8d85d1f1c7379ca754c39\n",
      "  Stored in directory: c:\\users\\sachin\\appdata\\local\\pip\\cache\\wheels\\9c\\3c\\41\\ea707c90a271e9f3cdc6f271420197142b0d815f2582280430\n",
      "  Building wheel for names (setup.py): started\n",
      "  Building wheel for names (setup.py): finished with status 'done'\n",
      "  Created wheel for names: filename=names-0.3.0-py3-none-any.whl size=803682 sha256=ed8c1ee34ae812add9cec043492a00e4345ba66f36c8743d1eeee87900359d8e\n",
      "  Stored in directory: c:\\users\\sachin\\appdata\\local\\pip\\cache\\wheels\\f9\\3e\\43\\632ba1676e3504ef23705d8402a5114c291fd825bb08480031\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15734 sha256=f03d3c1ac3f4265a5b2aaa7893d4de5a0ede5176bd894097bc77460737a0bfaa\n",
      "  Stored in directory: c:\\users\\sachin\\appdata\\local\\pip\\cache\\wheels\\c5\\10\\9a\\59753c0bfe5fe1a6f6253329e66b953962c9dc7e3201c20e0e\n",
      "  Building wheel for netifaces (setup.py): started\n",
      "  Building wheel for netifaces (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for netifaces\n",
      "Successfully built forbiddenfruit dpcontracts names sqlitedict\n",
      "Failed to build netifaces\n",
      "Installing collected packages: sqlitedict, PyJWT, netifaces, names, forbiddenfruit, av, win32-setctime, Werkzeug, websocket-client, torch, syft-proto, PyYAML, pyee, pyarrow, Jinja2, itsdangerous, google-crc32c, dpcontracts, dnspython, click, torchvision, requests-toolbelt, PyNaCl, pylibsrtp, loguru, flask, cryptography, aioice, pyOpenSSL, aiortc, syft\n",
      "  Running setup.py install for netifaces: started\n",
      "  Running setup.py install for netifaces: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "YWEaezUuWKaC",
    "outputId": "02f28cd3-0adb-49c3-b545-8d70e0c32b7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 09:58:33.568226 139995687696256 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0803 09:58:33.588890 139995687696256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import syft as sy\n",
    "import helper\n",
    "\n",
    "#Hooking syft to torch\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGQj7JfPZSil"
   },
   "outputs": [],
   "source": [
    "#Method to create 10 virtual workers and move to a list of workers\n",
    "def create_workers():\n",
    "  workers = []\n",
    "  cartman = sy.VirtualWorker(hook, id = \"cartman\")\n",
    "  workers.append(cartman)\n",
    "  kyle = sy.VirtualWorker(hook, id = \"kyle\")\n",
    "  workers.append(kyle)\n",
    "  kenny = sy.VirtualWorker(hook, id = \"kenny\")\n",
    "  workers.append(kenny)\n",
    "  stan = sy.VirtualWorker(hook, id = \"stan\")\n",
    "  workers.append(stan)\n",
    "  butters = sy.VirtualWorker(hook, id = \"butters\")\n",
    "  workers.append(butters)\n",
    "  wendy = sy.VirtualWorker(hook, id = \"wendy\")\n",
    "  workers.append(wendy)\n",
    "  heidi = sy.VirtualWorker(hook, id = \"heidi\")\n",
    "  workers.append(heidi)\n",
    "  bebe = sy.VirtualWorker(hook, id = \"bebe\")\n",
    "  workers.append(bebe)\n",
    "  nichole = sy.VirtualWorker(hook, id = \"nichole\")\n",
    "  workers.append(nichole)\n",
    "  patty = sy.VirtualWorker(hook, id = \"patty\")\n",
    "  workers.append(patty)\n",
    "  return workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_vP_5qrFfpe"
   },
   "outputs": [],
   "source": [
    "#Method to clear out every tensor stored in the list of virtual workers\n",
    "def clear_workers(workers):\n",
    "  for worker in workers:\n",
    "    worker.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpF-MD4EW_ON"
   },
   "outputs": [],
   "source": [
    "#Method to split the mnist test dataset into the various workers and also to load the mnist test dataset into a test loader\n",
    "def create_federated_and_test_loaders(workers, trainset, testset):\n",
    "  federated_train_loader = sy.FederatedDataLoader(\n",
    "      trainset.federate(workers), \n",
    "      batch_size=32, shuffle=True)\n",
    "\n",
    "  test_loader = th.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "  return federated_train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8xAwKFm9Vp0"
   },
   "outputs": [],
   "source": [
    "#Method to train models on the virtual workers without moving any gradient to the central models until the gradients have been coallated.\n",
    "def create_train_federated_models(workers, loader, lr = 0.12, epoch = 5):\n",
    "  #sends model to first virtual worker\n",
    "  virtual_model = classifier().send(workers[0])\n",
    "  optimizer = optim.SGD(virtual_model.parameters(), lr)\n",
    "  criterion = nn.NLLLoss()\n",
    "  for n in range(epoch):\n",
    "    \n",
    "    #Integer to keep up with first index.\n",
    "    i = 0\n",
    "    \n",
    "    #Integer to keep up with current worker while training\n",
    "    j = 0\n",
    "    \n",
    "    #Integer to count number of mini-batches per worker\n",
    "    n_mbatch = 0\n",
    "    \n",
    "    #Variable to keep up with current worker while looping\n",
    "    dbLoc = None\n",
    "    \n",
    "    #Variable to store cummulative loss.\n",
    "    cum_loss = 0\n",
    "    \n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
    "      \n",
    "      #condition to set dbLoc to the first worker\n",
    "      if i == 0:\n",
    "        i = 2\n",
    "        dbLoc = imgs.location\n",
    "        \n",
    "      #condition to change dbLoc if img is stored on a different worker and also calculate loss\n",
    "      if dbLoc is not imgs.location:\n",
    "        print(\"The total loss for {0} for epoch {2} is {1}\".format(workers[j].id, cum_loss / n_mbatch, n+1))\n",
    "        dbLoc = imgs.location\n",
    "        j += 1\n",
    "        \n",
    "        #Moving the model to a new worker\n",
    "        virtual_model.move(dbLoc)\n",
    "        \n",
    "        #Resetting the cummulative loss and batch count to zero for new worker\n",
    "        cum_loss = 0\n",
    "        n_mbatch = 0\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      output = virtual_model.forward(imgs)\n",
    "      loss = criterion(output, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      cum_loss +=  loss.get().item()\n",
    "      n_mbatch += 1\n",
    "    print(\"The total loss for {0} is {1}\".format(workers[j].id, cum_loss / n_mbatch))\n",
    "    \n",
    "    #Moving the model to the first worker if training would occur again\n",
    "    if (n < (epoch - 1)):\n",
    "      virtual_model.move(workers[0])\n",
    "  return virtual_model\n",
    "  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4FD2KugTeSn"
   },
   "outputs": [],
   "source": [
    "#Method to return the model to the central machine\n",
    "def create_central_model(model):\n",
    "  return model.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdzoIdyZUOQV"
   },
   "outputs": [],
   "source": [
    "#Method to analyze the private database with the trained model\n",
    "def analyze_model(model, loader):\n",
    "  print(\"Running on \", \"cpu\")\n",
    "  cum_perc = 0\n",
    "  for imgs, labels in loader:\n",
    "    with th.no_grad():\n",
    "      ps =  th.exp(model.forward(imgs))\n",
    "    top_p, top_class = ps.topk(1, dim = 1)\n",
    "    prob = top_class == labels.view(*top_class.shape)\n",
    "    prob = prob.float()\n",
    "    cum_perc += prob.mean().float()\n",
    "  print(\"The accuracy of the model is {0}%\".format((cum_perc / len(loader)) * 100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fA1Ox2FexyL"
   },
   "outputs": [],
   "source": [
    "#Classifier for creating the models\n",
    "class classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__() \n",
    "    self.fc1 = nn.Linear(784, 256)\n",
    "    self.fc2 = nn.Linear(256, 128)\n",
    "    self.fc3 = nn.Linear(128, 64)\n",
    "    self.fc4 = nn.Linear(64, 32)\n",
    "    self.fc5 = nn.Linear(32, 10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = F.relu(self.fc4(x))\n",
    "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
    "    return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UiMGYx2Wx9R"
   },
   "outputs": [],
   "source": [
    "# Application of transforms to normalize the mnist data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy7piyUUMUzT"
   },
   "outputs": [],
   "source": [
    "workers = create_workers()\n",
    "clear_workers(workers)\n",
    "federated_loader, test_loader = create_federated_and_test_loaders(workers, mnist_trainset, mnist_testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Jl17zwHVWePX",
    "outputId": "1d777500-1f67-4355-9cdc-591e5bdcb263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total loss for cartman for epoch 1 is 1.7833157254660383\n",
      "The total loss for kyle for epoch 1 is 0.9024252550716095\n",
      "The total loss for kenny for epoch 1 is 0.6204849152647435\n",
      "The total loss for stan for epoch 1 is 0.4085380588757231\n",
      "The total loss for butters for epoch 1 is 0.3886932226571631\n",
      "The total loss for wendy for epoch 1 is 0.35908689480671224\n",
      "The total loss for heidi for epoch 1 is 0.3309765687172717\n",
      "The total loss for bebe for epoch 1 is 0.29516828555534497\n",
      "The total loss for nichole for epoch 1 is 0.2855488977375183\n",
      "The total loss for patty is 0.174475033589183\n",
      "The total loss for cartman for epoch 2 is 0.22575426379099806\n",
      "The total loss for kyle for epoch 2 is 0.20347901956832154\n",
      "The total loss for kenny for epoch 2 is 0.2153895103392449\n",
      "The total loss for stan for epoch 2 is 0.17871475322766506\n",
      "The total loss for butters for epoch 2 is 0.18308901988921014\n",
      "The total loss for wendy for epoch 2 is 0.18084254954010248\n",
      "The total loss for heidi for epoch 2 is 0.17430340308458248\n",
      "The total loss for bebe for epoch 2 is 0.17721348134998946\n",
      "The total loss for nichole for epoch 2 is 0.15758031705751063\n",
      "The total loss for patty is 0.10357205752045551\n"
     ]
    }
   ],
   "source": [
    "virtual_model = create_train_federated_models(workers, federated_loader, epoch = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoVQPxPOT8jQ"
   },
   "outputs": [],
   "source": [
    "central_model = create_central_model(virtual_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LpaY4ULYUbvg",
    "outputId": "b2c5cb82-84fc-4c0d-ade7-7375d6887593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cpu\n",
      "The accuracy of the model is 96.1783447265625%\n"
     ]
    }
   ],
   "source": [
    "analyze_model(central_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FederatedLearning_Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9003e742f195946446e7171aa0bce8f0852e90101e1e8fc81c5c29dc8432471e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
